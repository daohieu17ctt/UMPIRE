{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437cc627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"./..\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d487709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755befd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f923c13d590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# Set a seed value\n",
    "seed_value = 10\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "#Fix torch random seed\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adf88f",
   "metadata": {},
   "source": [
    "## Load model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e095cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "max_new_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ba5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 10:46:25.077635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752547585.090902 3736010 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752547585.094777 3736010 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752547585.106802 3736010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752547585.106814 3736010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752547585.106815 3736010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752547585.106817 3736010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-15 10:46:25.110520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075e980014ba474badcdb57150983731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wrapper model\n",
    "if 'cogvlm' in model_path.lower():\n",
    "    from modules.models.cogvlm_models import CogVLMModel\n",
    "    model = CogVLMModel(model_name=model_path, stop_sequences=[], max_new_tokens=max_new_tokens)\n",
    "elif 'contactdoctor' in model_path.lower():\n",
    "    from modules.models.biomedllama_models import BioMedLlamaModel\n",
    "    model = BioMedLlamaModel(model_name=model_path, stop_sequences=[], max_new_tokens=max_new_tokens)\n",
    "elif 'liuhaotian' in model_path.lower():\n",
    "    from modules.models.llava_models import HuggingfaceModel as LlavaModel\n",
    "    model = LlavaModel(model_name=model_path, stop_sequences=[], max_new_tokens=max_new_tokens)\n",
    "else: # It can be any other model (LLava, Llama, etc)\n",
    "    from modules.models.vision_models import VisionModel\n",
    "    model = VisionModel(model_name=model_path, stop_sequences=[], max_new_tokens=max_new_tokens)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a4b87",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "\n",
    "The input data (image, text) should follow the format sth like this:\n",
    "- ```image_path = \"data/vqav2/val2014/COCO_val2014_000000525732.jpg\"```\n",
    "- ```question = \"What color strip does the surfboard have?\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7b6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VQAv2 dataset\n",
    "# Note: You can change the question_file and image_dir to your own dataset\n",
    "question_file = \"/home/daohieu/maplecg_nfs/research/VLM/su_vlm/data/vqav2/llava_OpenEnded_mscoco_val2014_questions.jsonl\"\n",
    "image_dir = \"/home/daohieu/maplecg_nfs/research/VLM/su_vlm/data/vqav2/val2014\"\n",
    "questions = [json.loads(q) for q in open(os.path.expanduser(question_file), \"r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb27c3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 262148000,\n",
       " 'image': 'COCO_val2014_000000262148.jpg',\n",
       " 'text': 'Where is he looking?',\n",
       " 'category': 'default',\n",
       " 'answers': ['down',\n",
       "  'down',\n",
       "  'at table',\n",
       "  'skateboard',\n",
       "  'down',\n",
       "  'table',\n",
       "  'down',\n",
       "  'down',\n",
       "  'down',\n",
       "  'down']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c29a14b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  0\n",
      "Image path: /home/daohieu/maplecg_nfs/research/VLM/su_vlm/data/vqav2/val2014/COCO_val2014_000000262148.jpg\n",
      "Question: Where is he looking?\n",
      "Answer: ['down', 'down', 'at table', 'skateboard', 'down', 'table', 'down', 'down', 'down', 'down']\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "image_path = os.path.join(image_dir, questions[idx][\"image\"])\n",
    "question = questions[idx][\"text\"]\n",
    "answer = questions[idx][\"answers\"]\n",
    "print(\"idx: \", idx)\n",
    "print(f\"Image path: {image_path}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69b84d",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "\n",
    "### U1: Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e10891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix prompt for the VQA task \n",
    "# You can change this prompt to fit your task\n",
    "prefix_prompt = 'Answer this question in only a word or a phrase. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6730426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prefix_prompt + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05884db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely generation output text: Above the table.\n"
     ]
    }
   ],
   "source": [
    "# Most likely generation with low temperature\n",
    "# This uses for evaluation purpose, i.e., to get the correctness of the model's prediction to evaluate UQ methods, these metrics can be AUROC, ECE, CPC etc.\n",
    "most_likely_generation_output_text, most_likely_generation_log_likelihood, most_likely_generation_embedding = model.predict_prompt_image(prompt, image_path, temperature=0.1, top_p=0.9)\n",
    "print(f\"Most likely generation output text: {most_likely_generation_output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ddbe7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  Above the table.\n",
      "Grouth truth:  ['down', 'down', 'at table', 'skateboard', 'down', 'table', 'down', 'down', 'down', 'down']\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: \", most_likely_generation_output_text)\n",
    "print(\"Grouth truth: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e220e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-samples generation with high temperature and save the log likelihood and embedding of the generation. \n",
    "# (this uses to compute UQ metrics)\n",
    "num_samples = 10\n",
    "temperature = 1.0\n",
    "top_p = 0.9\n",
    "generation_list = []\n",
    "generation_log_likelihood_list = []\n",
    "embedding = []\n",
    "for i in range(num_samples):\n",
    "    generation, generation_log_likelihood, generation_embedding = model.predict_prompt_image(prompt, image_path, temperature=temperature, top_p=top_p)\n",
    "    generation_list.append(generation)\n",
    "    generation_log_likelihood_list.append(generation_log_likelihood)\n",
    "    embedding.append(generation_embedding)\n",
    "embedding = np.array(torch.stack(embedding).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496fae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation list: ['above.', 'Above the table.', 'The ground.', 'The sky.', 'Above the skateboard.', 'Above the bench.', 'The ground.', 'Above the bench.', 'The skateboarder appears to be indoors.', 'Out of the window.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generation list: {generation_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9441b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (10, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding shape:\", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2ded11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihoods of a response: [-3.1181602478027344, -2.1971194744110107, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Log likelihoods of a response:\", generation_log_likelihood_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eba979",
   "metadata": {},
   "source": [
    "## Compute UMPIRE\n",
    "\n",
    "### U2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9f835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embedding(x):\n",
    "    return np.array([e / np.linalg.norm(e, ord=2) for e in x])\n",
    "\n",
    "def compute_incoherence_score(x):\n",
    "    x_ = 1-np.array([np.exp(np.sum(i)) for i in x]) # 1-p_sequence\n",
    "    return x_ \n",
    "\n",
    "from sklearn.utils.extmath import fast_logdet\n",
    "def compute_logdet(K, alpha=1e-8):\n",
    "    # seed = np.random.rand()\n",
    "    logdet_value = fast_logdet(K + np.identity(K.shape[0])*alpha)\n",
    "    return logdet_value\n",
    "\n",
    "def compute_umpire(V, C, alpha=30):\n",
    "    \"\"\"\n",
    "    Compute the UMPIRE value.\n",
    "    V: logdet value\n",
    "    C: incoherence score\n",
    "    alpha: hyperparameter\n",
    "    \"\"\"\n",
    "    V_tilde = V + alpha * np.linalg.norm(C, ord=1)\n",
    "    return V_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39efed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized embedding vector $\\phi$ shape: (10, 4096)\n",
      "Incoherence score C: [0.9950841  0.92494653 0.9538083  0.96402587 0.98102351 0.94508971\n",
      " 0.9538083  0.94508971 0.99997448 0.99974541]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the embedding\n",
    "norm_embedding = normalize_embedding(embedding)\n",
    "\n",
    "# Phi is the normalized embedding\n",
    "phi = norm_embedding\n",
    "print(r\"Normalized embedding vector $\\phi$ shape:\", phi.shape)\n",
    "\n",
    "# Incoherence score\n",
    "C = compute_incoherence_score(generation_log_likelihood_list)\n",
    "print(\"Incoherence score C:\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09ef0fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMPIRE V_tilde: 232.42780981658478\n"
     ]
    }
   ],
   "source": [
    "# Logdet \n",
    "V = compute_logdet(np.matmul(phi, phi.T))\n",
    "\n",
    "# UMPIRE\n",
    "alpha = 30\n",
    "V_tilde = compute_umpire(V, C, alpha)\n",
    "print(\"UMPIRE V_tilde:\", V_tilde) # Range of V_tilde should be from (-inf, 2xx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
